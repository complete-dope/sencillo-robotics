{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILE_PATH = os.getcwd()\n",
    "print(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866ae6a",
   "metadata": {},
   "source": [
    "The idea / motivation is to get a nice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9470298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipConfig, SiglipModel\n",
    "\n",
    "config = SiglipConfig()\n",
    "model = SiglipModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d932a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Vision encoder\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.vision_encoder = model.vision_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> image tensor (ch , H,  W)\n",
    "\n",
    "        x = self.vision_encoder(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "# text encoder\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.text_encoder = model.text_model\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.text_encoder(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aaae8d",
   "metadata": {},
   "source": [
    "## SigLIP model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c4e577",
   "metadata": {},
   "source": [
    "Vision Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0dd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Model  \n",
    "\n",
    "from transformers import Siglip2VisionConfig, Siglip2VisionModel\n",
    "\n",
    "Vision_config = Siglip2VisionConfig()\n",
    "model = Siglip2VisionModel(Vision_config)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4740e",
   "metadata": {},
   "source": [
    "Text Model\n",
    "\n",
    "--- \n",
    "This outputs 2 things \n",
    "\n",
    "1. last hidden state \n",
    "2. pooler output \n",
    "\n",
    "So the last hidden state this tells the token wise embedding for each token in the input and is useful for token level tasks\n",
    "\n",
    "Pooler output is the single vector representing the entire sequence , so for VLM models this is usefule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Model \n",
    "\n",
    "from transformers import Siglip2TextConfig, Siglip2TextModel\n",
    "\n",
    "text_config = Siglip2TextConfig()\n",
    "model = Siglip2TextModel(text_config)\n",
    "\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
