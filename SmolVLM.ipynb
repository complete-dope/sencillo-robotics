{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5d0c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mohitdulani/Desktop/personal/sencillo-robotics\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILE_PATH = os.getcwd()\n",
    "print(FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323088f2",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2504.05299 -> amazing vlm paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59801c60",
   "metadata": {},
   "source": [
    "![SmolVLM](assets/images/smolvlm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4f29b",
   "metadata": {},
   "source": [
    "---\n",
    "The first half is same as what we did for ViT paper ! just that the backbon is of CLIP model\n",
    "\n",
    "\n",
    "CLIP outputs 512 dim features \n",
    "and so does the its text embeddings \n",
    "\n",
    "---\n",
    "\n",
    "But we are gonna use the SigLIP instead of CLIP lets make the architecture !  \n",
    "\n",
    "So, lets start with it , the concatenation of the vision and text tokens and passing to a LM model \n",
    "\n",
    "\n",
    "Vision tokens : pass it to SigLIP image backbone , then get reshape the `last_hidden_state` to the H x W x C and then try to reduce the no. of tokens using pixel shuffling , once tokens are reduced pass that to the linear layer and the output from the linear layer to any GPT based LM (language model) model and you get your output token stream !! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7750c",
   "metadata": {},
   "source": [
    "### Vision encoder\n",
    "this is also taken from the siglip model \n",
    "\n",
    "\n",
    "Outputs : \n",
    "1. last hidden state\n",
    "2. pooler output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d89afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiglipVisionModel(\n",
      "  (vision_model): SiglipVisionTransformer(\n",
      "    (embeddings): SiglipVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
      "      (position_embedding): Embedding(196, 768)\n",
      "    )\n",
      "    (encoder): SiglipEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x SiglipEncoderLayer(\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (self_attn): SiglipAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): SiglipMLP(\n",
      "            (activation_fn): PytorchGELUTanh()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): SiglipMultiheadAttentionPoolingHead(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): SiglipMLP(\n",
      "        (activation_fn): PytorchGELUTanh()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import SiglipVisionConfig, SiglipVisionModel\n",
    "\n",
    "# Initializing a SiglipVisionConfig with google/siglip-base-patch16-224 style configuration\n",
    "configuration = SiglipVisionConfig()\n",
    "\n",
    "# Initializing a SiglipVisionModel (with random weights) from the google/siglip-base-patch16-224 style configuration\n",
    "model = SiglipVisionModel(configuration)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1370fec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.randn(size= (3, 224, 224))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd98abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_patches = 196   # 14x14\n",
    "patch_dim = 768\n",
    "llm_dim = 1024\n",
    "grid_size = 14\n",
    "vit_dim = patch_dim\n",
    "upscale_factor = 2  # e.g., merge 2x2 patches into one super-patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0956496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output features \n",
    "image = torch.randn(size= (1 ,3, 224, 224))\n",
    "\n",
    "# Vision model \n",
    "patch_embeddings = model(image).last_hidden_state\n",
    "\n",
    "# --- Step 2: Reshape to grid for PixelShuffle ---\n",
    "grid_size = int(num_patches ** 0.5)  # 14\n",
    "x = patch_embeddings.view(batch_size, grid_size, grid_size, vit_dim)  # [B, 14, 14, 768]\n",
    "x = x.permute(0, 3, 1, 2)  # [B, 768, 14, 14]\n",
    "\n",
    "# --- Step 3: Pixel Shuffle ---\n",
    "out_channels = vit_dim // (upscale_factor**2)  # 768 / 4 = 192\n",
    "x = x[:, :out_channels * (upscale_factor**2), :, :]  # ensure divisible\n",
    "pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "x = pixel_shuffle(x)  # [B, 192, 28, 28]\n",
    "\n",
    "# --- Step 4: Flatten back to sequence ---\n",
    "B, C, H, W = x.shape\n",
    "x = x.permute(0, 2, 3, 1).reshape(B, H*W, C)  # [B, 784, 192]\n",
    "\n",
    "# --- Step 5: Linear projection to LLM embedding size ---\n",
    "linear_proj = nn.Linear(C, llm_dim)\n",
    "vision_tokens_for_lm = linear_proj(x)  # [B, 784, 1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b83b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(vision_tokens_for_lm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ca3ba",
   "metadata": {},
   "source": [
    "### Text Encoder\n",
    "\n",
    "this is also taken from the SigLIP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe81883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipTextConfig , SiglipTextModel\n",
    "\n",
    "config = SiglipTextConfig()\n",
    "model = SiglipTextModel(config)\n",
    "tokenizer = \n",
    "\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
