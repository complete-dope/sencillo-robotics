{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101c3221",
   "metadata": {},
   "source": [
    "In the CLIP paper we dont take the i_k dim vs the t_k+x dimension because each dimension be it in text modality or image modality it should take up the same meaning , a dimension 'k' should represent the same in both the vectors as they are coming from the same latent space !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6815aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mohitdulani/Desktop/personal/sencillo-robotics\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILE_PATH = os.getcwd()\n",
    "print(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063c88d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dac0d6bbad40ef9d1332e1c0ae5171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322d3e6f433b46c988ded9a3505fa2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# This is a ViT model \n",
    "from datasets import load_dataset\n",
    "from PIL import Image  # force Pillow import\n",
    "\n",
    "\n",
    "# lets take a small image \n",
    "# ds = load_dataset(\"itsmohit/stl10_5percent\", split='test',  cache_dir=FILE_PATH, )\n",
    "ds = load_dataset('mnist', split='train[:100]', cache_dir = FILE_PATH)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb689b7c",
   "metadata": {},
   "source": [
    "![CLIP Model](assets/images/clip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f6da2",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "CLIP ( Contrastive Language Image Pretraining  )\n",
    "\n",
    "1. Image - text retrieval ( finding the caption (not generating it) ) \n",
    "2. Zero Shot Classification, acts as a classifier ( A Image of dog, Image of cat .. )\n",
    "3. Feature Extraction ( its anyways a ViT backbone on trained text space as well )\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "**Vision Model** : So the vision encoder is same as the ViT backbone it takes in the image , makes patches then converts them to tokens that's the encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55e704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14c55fe1b4c444f9d798a76551ec2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "      (position_embedding): Embedding(197, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Here we need an image encoder to extract the features from the images (Can use resnet as well as ViT) also we need and text encoder (transformers rules best) and then use it to joinly train both ! \n",
    "\n",
    "\n",
    "from transformers import CLIPConfig, CLIPModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "# 1️⃣ Load the config only (does not download weights)\n",
    "config = CLIPConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2️⃣ Initialize the model with random weights\n",
    "model = CLIPModel(config)\n",
    "\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vision encoder\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.vision_encoder = model.vision_model \n",
    "        self.vision_projection = model.visual_projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> image tensor (ch , H,  W)\n",
    "\n",
    "        x = self.vision_encoder(x)\n",
    "        x = self.vision_projection(x) \n",
    "        return x \n",
    "\n",
    "\n",
    "# text encoder\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.text_encoder = model.text_model\n",
    "        self.text_projection = model.text_projection\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.text_encoder(x)\n",
    "        x = self.text_projection(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93de388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(model.forward))\n",
    "# The actual code for how the forward method works in these models !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c1925",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "---\n",
    "\n",
    "Image_embedding : BackBone Image extractor model i.e. [n , d_e]  \n",
    "Text_embedding  : BackBone Text extractor model i.e. [n , d_e]\n",
    "\n",
    "--- \n",
    "\n",
    "logits_before = np.dot(I_e, T_e.T) \n",
    "logits = logits_before * np.exp(t) ## why ? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
