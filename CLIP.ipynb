{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101c3221",
   "metadata": {},
   "source": [
    "In the CLIP paper we dont take the i_k dim vs the t_k+x dimension because each dimension be it in text modality or image modality it should take up the same meaning , a dimension 'k' should represent the same in both the vectors as they are coming from the same latent space !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILE_PATH = os.getcwd()\n",
    "print(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a ViT model \n",
    "from datasets import load_dataset\n",
    "from PIL import Image  # force Pillow import\n",
    "\n",
    "\n",
    "# lets take a small image \n",
    "# ds = load_dataset(\"itsmohit/stl10_5percent\", split='test',  cache_dir=FILE_PATH, )\n",
    "ds = load_dataset('mnist', split='train[:100]', cache_dir = FILE_PATH)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb689b7c",
   "metadata": {},
   "source": [
    "![CLIP Model](assets/images/clip.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55e704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14c55fe1b4c444f9d798a76551ec2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "      (position_embedding): Embedding(197, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Here we need an image encoder to extract the features from the images (Can use resnet as well as ViT) also we need and text encoder (transformers rules best) and then use it to joinly train both ! \n",
    "\n",
    "\n",
    "from transformers import CLIPConfig, CLIPModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "# 1️⃣ Load the config only (does not download weights)\n",
    "config = CLIPConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2️⃣ Initialize the model with random weights\n",
    "model = CLIPModel(config)\n",
    "\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vision encoder\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.vision_encoder = model.vision_model \n",
    "        self.vision_projection = model.visual_projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> image tensor (ch , H,  W)\n",
    "\n",
    "        x = self.vision_encoder(x)\n",
    "        x = self.vision_projection(x) \n",
    "        return x \n",
    "\n",
    "\n",
    "# text encoder\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.text_encoder = model.text_model\n",
    "        self.text_projection = model.text_projection\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.text_encoder(x)\n",
    "        x = self.text_projection(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93de388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(model.forward))\n",
    "# The actual code for how the forward method works in these models !! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
